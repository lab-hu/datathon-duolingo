{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2527e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: duckdb in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.4.4)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark pandas numpy matplotlib seaborn duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e1ff9",
   "metadata": {},
   "source": [
    "# Duolingo Datathon - EDA with PySpark\n",
    "## DatasetC: Sleeping, Recovering Bandit Algorithm for Notifications\n",
    "Working with training dataset (~87M rows, 4GB extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7c3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8c9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "WARNING: package sun.security.action not in java.base\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/19 15:09:10 WARN Utils: Your hostname, codespaces-80ea76, resolves to a loopback address: 127.0.0.1; using 10.0.0.39 instead (on interface eth0)\n",
      "26/02/19 15:09:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/19 15:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING: A terminally deprecated method in sun.misc.Unsafe has been called\n",
      "WARNING: sun.misc.Unsafe::arrayBaseOffset has been called by org.apache.spark.unsafe.Platform (file:/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/jars/spark-unsafe_2.13-4.1.1.jar)\n",
      "WARNING: Please consider reporting this to the maintainers of class org.apache.spark.unsafe.Platform\n",
      "WARNING: sun.misc.Unsafe::arrayBaseOffset will be removed in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.1.1\n",
      "Driver Memory: 4g\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"duolingo_datathon\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Driver Memory: {spark.sparkContext.getConf().get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f96370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 training data partitions:\n",
      "  - /workspaces/datathon-duolingo/train-part-1\n",
      "  - /workspaces/datathon-duolingo/train-part-2\n",
      "  - /workspaces/datathon-duolingo/train-part-3\n"
     ]
    }
   ],
   "source": [
    "# Find all training directories (not individual parquet files)\n",
    "train_dir = \"/workspaces/datathon-duolingo\"\n",
    "train_parts = sorted([d for d in glob.glob(f\"{train_dir}/train-part-*\") if os.path.isdir(d)])\n",
    "print(f\"Found {len(train_parts)} training data partitions:\")\n",
    "for tp in train_parts:\n",
    "    print(f\"  - {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa55fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parquet: /workspaces/datathon-duolingo/train-part-1/part-00000-9b4bba6b-feac-44b1-a155-17c796835cca-c000.snappy.parquet\n",
      "\n",
      "✓ Successfully read! Shape: (3, 6)\n",
      "\n",
      "Columns: ['datetime', 'ui_language', 'eligible_templates', 'history', 'selected_template', 'session_end_completed']\n",
      "\n",
      "First 3 rows:\n",
      "   datetime ui_language              eligible_templates  \\\n",
      "0  0.153461          en  [G, E, B, A, K, H, J, L, F, D]   \n",
      "1  2.827303          es  [G, E, B, A, K, H, J, L, F, D]   \n",
      "2  2.792662          en     [G, E, B, K, H, J, L, F, D]   \n",
      "\n",
      "                                             history selected_template  \\\n",
      "0  [{'template': 'A', 'n_days': 28.19564819335937...                 B   \n",
      "1  [{'template': 'A', 'n_days': 29.836181640625},...                 A   \n",
      "2  [{'template': 'G', 'n_days': 8.197543144226074...                 J   \n",
      "\n",
      "   session_end_completed  \n",
      "0                  False  \n",
      "1                   True  \n",
      "2                   True  \n"
     ]
    }
   ],
   "source": [
    "# Quick test: read one parquet file with DuckDB to verify integrity\n",
    "import duckdb\n",
    "import glob\n",
    "\n",
    "test_file = glob.glob(\"/workspaces/datathon-duolingo/train-part-1/*.parquet\")[0]\n",
    "print(f\"Testing parquet: {test_file}\")\n",
    "\n",
    "try:\n",
    "    conn = duckdb.connect()\n",
    "    result = conn.execute(f\"SELECT * FROM read_parquet('{test_file}') LIMIT 3\").df()\n",
    "    print(f\"\\n✓ Successfully read! Shape: {result.shape}\")\n",
    "    print(f\"\\nColumns: {result.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd782321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 parquet files total\n",
      "Files:\n",
      "  - /workspaces/datathon-duolingo/train-part-1/part-00000-9b4bba6b-feac-44b1-a155-17c796835cca-c000.snappy.parquet\n",
      "  - /workspaces/datathon-duolingo/train-part-2/part-00001-9b4bba6b-feac-44b1-a155-17c796835cca-c000.snappy.parquet\n",
      "  - /workspaces/datathon-duolingo/train-part-3/part-00002-9b4bba6b-feac-44b1-a155-17c796835cca-c000.snappy.parquet\n",
      "\n",
      "✓ Total rows across all partitions: 87,665,839\n",
      "\n",
      "Loading sample (10K rows per partition = 30K total)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded sample: 30,000 rows\n",
      "\n",
      "Columns: ['datetime', 'ui_language', 'eligible_templates', 'history', 'selected_template', 'session_end_completed']\n",
      "\n",
      "Data types:\n",
      "datetime                 float64\n",
      "ui_language                  str\n",
      "eligible_templates        object\n",
      "history                   object\n",
      "selected_template            str\n",
      "session_end_completed       bool\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n",
      "    datetime ui_language              eligible_templates  \\\n",
      "0   5.673044          es  [G, E, B, A, K, H, J, L, F, D]   \n",
      "1   3.281991          en     [G, E, B, K, H, J, L, F, D]   \n",
      "2  12.006505          en  [K, H, G, E, B, J, L, F, D, A]   \n",
      "3  13.067569          vi     [K, H, G, E, B, J, L, F, D]   \n",
      "4   1.970174          en     [G, E, B, K, H, J, L, F, D]   \n",
      "\n",
      "                                             history selected_template  \\\n",
      "0                                                 []                 B   \n",
      "1  [{'template': 'F', 'n_days': 1.999998569488525...                 K   \n",
      "2                                                 []                 L   \n",
      "3  [{'template': 'D', 'n_days': 5.000000476837158...                 J   \n",
      "4  [{'template': 'G', 'n_days': 1.999994874000549...                 F   \n",
      "\n",
      "   session_end_completed  \n",
      "0                  False  \n",
      "1                  False  \n",
      "2                  False  \n",
      "3                  False  \n",
      "4                  False  \n"
     ]
    }
   ],
   "source": [
    "# Load ALL 3 partitions with DuckDB (memory-efficient queries)\n",
    "import duckdb\n",
    "import glob\n",
    "\n",
    "# Find all parquet files\n",
    "parquet_files = sorted(glob.glob(\"/workspaces/datathon-duolingo/train-part-*/part-*.parquet\"))\n",
    "print(f\"Found {len(parquet_files)} parquet files total\")\n",
    "print(\"Files:\")\n",
    "for f in parquet_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "try:\n",
    "    # Create DuckDB connection\n",
    "    conn = duckdb.connect()\n",
    "    \n",
    "    # Get total row count across ALL partitions (fast - reads metadata only)\n",
    "    total_rows = conn.execute(f\"\"\"\n",
    "        SELECT COUNT(*) as total_rows \n",
    "        FROM read_parquet({parquet_files})\n",
    "    \"\"\").fetchone()[0]\n",
    "    \n",
    "    print(f\"\\n✓ Total rows across all partitions: {total_rows:,}\")\n",
    "    \n",
    "    # Option 1: Load a SAMPLE from all partitions (memory-safe)\n",
    "    # This takes 10,000 rows from EACH file = 30,000 total\n",
    "    print(\"\\nLoading sample (10K rows per partition = 30K total)...\")\n",
    "    df_train = conn.execute(f\"\"\"\n",
    "        SELECT * FROM read_parquet({parquet_files})\n",
    "        USING SAMPLE 30000\n",
    "    \"\"\").df()\n",
    "    \n",
    "    print(f\"✓ Loaded sample: {len(df_train):,} rows\")\n",
    "    print(f\"\\nColumns: {df_train.columns.tolist()}\")\n",
    "    print(f\"\\nData types:\\n{df_train.dtypes}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df_train.head())\n",
    "    \n",
    "    # NOTE: To load FULL dataset (all 87M rows), replace above query with:\n",
    "    # df_train = conn.execute(f\"SELECT * FROM read_parquet({parquet_files})\").df()\n",
    "    # WARNING: This uses ~2-4GB RAM and might crash kernel\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab338fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics (pandas-based)\n",
    "print(\"=== DATA PROFILING ===\\n\")\n",
    "\n",
    "# Row count and memory usage\n",
    "print(f\"Total rows in sample: {len(df_train):,}\")\n",
    "print(f\"Number of columns: {len(df_train.columns)}\")\n",
    "print(f\"Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Null values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Column data types\n",
    "print(\"\\nData types:\")\n",
    "print(df_train.dtypes)\n",
    "\n",
    "\n",
    "# Basic statistics for numeric columnsprint(df_train.describe())\n",
    "print(\"\\nNumeric column statistics:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis (pandas-based)\n",
    "print(\"=== TARGET VARIABLE ANALYSIS ===\\n\")\n",
    "\n",
    "# Session completion rate\n",
    "completion_counts = df_train['session_end_completed'].value_counts()\n",
    "print(\"Session completion distribution:\")\n",
    "print(completion_counts)\n",
    "\n",
    "completion_pct = (completion_counts / completion_counts.sum() * 100)\n",
    "print(f\"\\nCompletion rate (True): {completion_pct.get(True, 0):.2f}%\")\n",
    "print(f\"Non-completion rate (False): {completion_pct.get(False, 0):.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "completion_counts.plot(kind='bar', color=['#fc8d62', '#66c2a5'])\n",
    "plt.title('Session Completion Distribution (Sample)')\n",
    "plt.xlabel('Session Completed (2h window)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
